{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 2","language":"python","name":"python2"},"language_info":{"codemirror_mode":{"name":"ipython","version":2},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython2","version":"2.7.13"},"colab":{"provenance":[],"collapsed_sections":["ihX3gbMNRN6k","5OuvxuzuRN6l"]}},"cells":[{"cell_type":"markdown","metadata":{"id":"Op2yF_XGRN5D"},"source":["# Feature Selection for ML"]},{"cell_type":"markdown","metadata":{"id":"3Q9aUfG6RN5E"},"source":["We discuss feature selection techniques that you can use to prepare your ML data in Python with scikit-learn.\n","\n","Focus will be on:\n","1. Univariate Selection\n","2. Recursive Feature Elimination\n","3. Feature Importance\n","\n","Then we look at another approach: Principal Component Analysis (not plenty of details in this part of the course)"]},{"cell_type":"markdown","metadata":{"id":"uR9VV3FARN5F"},"source":["## Introduction"]},{"cell_type":"markdown","metadata":{"id":"3ubyEy6FRN5G"},"source":["Keep an eye, in the following, to 3 possible goals/benefits of a clever feature selection tactics:\n","\n","1. ***Reduction of Overfitting***\n","1. ***Improvement of Accuracy***\n","1. ***Reduction of Training Time***\n"]},{"cell_type":"markdown","metadata":{"id":"rsBL2p7Mpo7W"},"source":["\n","More about feature selection with scikit-learn can be found [here](http://scikit-learn.org/stable/modules/feature_selection.html)."]},{"cell_type":"markdown","metadata":{"id":"6NyHWASMpbyq"},"source":["## 0. Import the data"]},{"cell_type":"code","metadata":{"id":"egGmPYr6pblR"},"source":["import pandas as pd\n","\n","url = 'https://raw.githubusercontent.com/dbonacorsi/AMLBas2324/main/pima-indians-diabetes.data.csv'\n","\n","names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n","data = pd.read_csv(url, names=names)\n","data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yPZ8swohksne"},"source":["data.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Kg2NJcPkRN5H"},"source":["## 1. Univariate Selection"]},{"cell_type":"markdown","metadata":{"id":"BaoeTaNARN5I"},"source":["The scikit-learn library provides the `SelectKBest` class (info [here](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html#sklearn.feature_selection.SelectKBest)) that can be used with a suite of different statistical tests to select a specific number of features. Apart from `SelectKBest`, you may use `SelectPercentile` or `GenericUnivariateSelect` (check documentation).\n","\n","The example below uses the chi-squared (chi2) statistical test for non-negative features to select 4 of the best features from the Pima Indians onset of diabetes dataset."]},{"cell_type":"code","metadata":{"id":"evZ0LUW9RN5K","executionInfo":{"status":"ok","timestamp":1711081102524,"user_tz":-60,"elapsed":1490,"user":{"displayName":"Daniele Bonacorsi","userId":"01397661520201218305"}}},"source":["from numpy import set_printoptions\n","\n","from sklearn.feature_selection import SelectKBest\n","from sklearn.feature_selection import chi2"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"eTs-DqyQRN5P","executionInfo":{"status":"ok","timestamp":1711081104266,"user_tz":-60,"elapsed":236,"user":{"displayName":"Daniele Bonacorsi","userId":"01397661520201218305"}}},"source":["array = data.values\n","X = array[:,0:8]\n","Y = array[:,8]"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"xavljlu0lAR3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1711081108222,"user_tz":-60,"elapsed":284,"user":{"displayName":"Daniele Bonacorsi","userId":"01397661520201218305"}},"outputId":"9647c066-cb55-4cf6-db3c-fdf2865d29eb"},"source":["X.shape"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(768, 8)"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"jbOHslallCfW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1711081109591,"user_tz":-60,"elapsed":5,"user":{"displayName":"Daniele Bonacorsi","userId":"01397661520201218305"}},"outputId":"fa055157-9e9a-4c8d-d91d-3082ddf83179"},"source":["Y.shape"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(768,)"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"Z17GQmcbRN5T","executionInfo":{"status":"ok","timestamp":1711081128804,"user_tz":-60,"elapsed":268,"user":{"displayName":"Daniele Bonacorsi","userId":"01397661520201218305"}}},"source":["# chi2 to select the best k=..\n","test = SelectKBest(score_func=chi2, k=3)\n","fit = test.fit(X, Y)"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"-xXadTD4RN5X","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1711081131900,"user_tz":-60,"elapsed":249,"user":{"displayName":"Daniele Bonacorsi","userId":"01397661520201218305"}},"outputId":"93a9ac6f-0337-412e-c707-d3ab75dfdf5d"},"source":["# summarize scores\n","set_printoptions(precision=3)\n","print(fit.scores_)"],"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["[ 111.52  1411.887   17.605   53.108 2175.565  127.669    5.393  181.304]\n"]}]},{"cell_type":"code","metadata":{"id":"g8AVEa39RN5f","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1711081139135,"user_tz":-60,"elapsed":350,"user":{"displayName":"Daniele Bonacorsi","userId":"01397661520201218305"}},"outputId":"82b3faa3-da27-4b4f-8c0f-77252174be25"},"source":["names[:-1]"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age']"]},"metadata":{},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"lNl3O2YFRN5j"},"source":["So, the **k(=3) best ones** seems to be: **plas, test** (and perhaps **age**, but much worse)"]},{"cell_type":"markdown","metadata":{"id":"oFY3cxqIlNR4"},"source":["[NOTE]: If you want to transform it, you see that change of shape.. (careful, this is powerful..)"]},{"cell_type":"code","metadata":{"id":"anIiC5delLnZ","executionInfo":{"status":"ok","timestamp":1711081239134,"user_tz":-60,"elapsed":4,"user":{"displayName":"Daniele Bonacorsi","userId":"01397661520201218305"}}},"source":["X_new = SelectKBest(score_func=chi2, k=3).fit_transform(X, Y)"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"GQduw3nklLkO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1711081242014,"user_tz":-60,"elapsed":836,"user":{"displayName":"Daniele Bonacorsi","userId":"01397661520201218305"}},"outputId":"2535d34c-df27-4b39-d682-e06f8fa8f4da"},"source":["X_new.shape"],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(768, 3)"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"v9xUrPpjlLhZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1711081245611,"user_tz":-60,"elapsed":278,"user":{"displayName":"Daniele Bonacorsi","userId":"01397661520201218305"}},"outputId":"ccb6c248-7128-4ab4-9e8a-3276e34b71d9"},"source":["X_new"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[148.,   0.,  50.],\n","       [ 85.,   0.,  31.],\n","       [183.,   0.,  32.],\n","       ...,\n","       [121., 112.,  30.],\n","       [126.,   0.,  47.],\n","       [ 93.,   0.,  23.]])"]},"metadata":{},"execution_count":12}]},{"cell_type":"markdown","metadata":{"id":"UQ8cZXbYlz3p"},"source":["Let's go back on track."]},{"cell_type":"code","metadata":{"scrolled":true,"id":"q6IpDyssRN5k","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1711081253868,"user_tz":-60,"elapsed":263,"user":{"displayName":"Daniele Bonacorsi","userId":"01397661520201218305"}},"outputId":"691a11b4-4eec-4e32-bf58-334bfb6c9265"},"source":["# summarize selected features\n","X_new1 = fit.transform(X)\n","print(X_new1[0:10,:])"],"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["[[148.   0.  50.]\n"," [ 85.   0.  31.]\n"," [183.   0.  32.]\n"," [ 89.  94.  21.]\n"," [137. 168.  33.]\n"," [116.   0.  30.]\n"," [ 78.  88.  26.]\n"," [115.   0.  29.]\n"," [197. 543.  53.]\n"," [125.   0.  54.]]\n"]}]},{"cell_type":"markdown","metadata":{"id":"tRlOWtD7mU_I"},"source":["You can do all the above in one shot with `fit_transform` (careful, often this is dangerous..)"]},{"cell_type":"code","metadata":{"id":"7waiPBLXmU7z","executionInfo":{"status":"ok","timestamp":1711081263441,"user_tz":-60,"elapsed":3,"user":{"displayName":"Daniele Bonacorsi","userId":"01397661520201218305"}},"outputId":"9de5799b-0608-4d6b-901c-4cd4fab92296","colab":{"base_uri":"https://localhost:8080/"}},"source":["X_new2 = SelectKBest(score_func=chi2, k=3).fit_transform(X,Y)\n","print(X_new2[0:10,:])"],"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["[[148.   0.  50.]\n"," [ 85.   0.  31.]\n"," [183.   0.  32.]\n"," [ 89.  94.  21.]\n"," [137. 168.  33.]\n"," [116.   0.  30.]\n"," [ 78.  88.  26.]\n"," [115.   0.  29.]\n"," [197. 543.  53.]\n"," [125.   0.  54.]]\n"]}]},{"cell_type":"markdown","metadata":{"id":"cj-JUxDxRN5u"},"source":["## 2. Recursive Feature Elimination"]},{"cell_type":"markdown","metadata":{"id":"8SSPSZcLRN5v"},"source":["\n","More about the RFE class in the scikit-learn documentation [here](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html#sklearn.feature_selection.RFE)."]},{"cell_type":"code","metadata":{"id":"FP-jW8WKRN5w","executionInfo":{"status":"ok","timestamp":1711081301067,"user_tz":-60,"elapsed":246,"user":{"displayName":"Daniele Bonacorsi","userId":"01397661520201218305"}}},"source":["from sklearn.feature_selection import RFE\n","from sklearn.linear_model import LogisticRegression    # <---- note from where we take this module..."],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"Oc1qtNuiRN50","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1711081342168,"user_tz":-60,"elapsed":7,"user":{"displayName":"Daniele Bonacorsi","userId":"01397661520201218305"}},"outputId":"0342b50a-6eb7-4e48-8ed7-4a4dd74cfea6"},"source":["#model = LogisticRegression()\n","model = LogisticRegression(solver='lbfgs', max_iter=5000)    # <---- how we train\n","#rfe = RFE(model, 3)    # this started to give error in recent sklearn versions\n","rfe = RFE(model, n_features_to_select=3)   # my choice: seek for 3 features\n","fit = rfe.fit(X, Y)\n","print(\"Num Features: %d\" % fit.n_features_)\n","print(\"Selected Features: %s\" % fit.support_)\n","print(\"Feature Ranking: %s\" % fit.ranking_)"],"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Num Features: 3\n","Selected Features: [ True False False False False  True  True False]\n","Feature Ranking: [1 2 4 6 5 1 1 3]\n"]}]},{"cell_type":"code","metadata":{"scrolled":false,"id":"JlPDPjo0RN53","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1711081362982,"user_tz":-60,"elapsed":233,"user":{"displayName":"Daniele Bonacorsi","userId":"01397661520201218305"}},"outputId":"d76ed9c1-d2e8-4975-c41a-9b4dd393518b"},"source":["names[:-1]"],"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age']"]},"metadata":{},"execution_count":20}]},{"cell_type":"markdown","metadata":{"id":"ErQenA9ZRN57"},"source":["You can see that RFE has chosen the **top 3 features** as **preg, mass, pedi**."]},{"cell_type":"markdown","metadata":{"id":"IO_oKcAkRN59"},"source":["## 3. Feature Importance"]},{"cell_type":"markdown","metadata":{"id":"tQRj9sCKRN59"},"source":["\n","More about the ExtraTreesClassifier class can be found in the scikit-learn API [here](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html)."]},{"cell_type":"code","metadata":{"id":"juED2qQFRN5-","executionInfo":{"status":"ok","timestamp":1711081398285,"user_tz":-60,"elapsed":271,"user":{"displayName":"Daniele Bonacorsi","userId":"01397661520201218305"}}},"source":["from sklearn.ensemble import ExtraTreesClassifier    # <---- note from where we take this module, instead!"],"execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"id":"t9qC1hbGRN6B","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1711081400181,"user_tz":-60,"elapsed":825,"user":{"displayName":"Daniele Bonacorsi","userId":"01397661520201218305"}},"outputId":"edb739b3-b657-4fcf-8135-ab2267cde7aa"},"source":["# Feature Importance with Extra Trees Classifier\n","model = ExtraTreesClassifier()\n","#model = ExtraTreesClassifier(n_estimators=100)\n","model.fit(X, Y)\n","print(model.feature_importances_)"],"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.108 0.223 0.1   0.084 0.078 0.14  0.118 0.149]\n"]}]},{"cell_type":"code","metadata":{"id":"yOzdbD0URN6F","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1711081403512,"user_tz":-60,"elapsed":235,"user":{"displayName":"Daniele Bonacorsi","userId":"01397661520201218305"}},"outputId":"49a23c78-3416-47d8-8d58-3aef0a18a7ee"},"source":["names[:-1]"],"execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age']"]},"metadata":{},"execution_count":23}]},{"cell_type":"markdown","metadata":{"id":"hdFqygZMRN6I"},"source":["**The larger the score, the more important the attribute**. The scores suggest at the importance of **plas** and perhaps also **mass, age**."]},{"cell_type":"markdown","metadata":{"id":"yqG9Uyk8RN6L"},"source":["## Another approach: Principal Component Analysis"]},{"cell_type":"markdown","metadata":{"id":"ywrBymoVRN6M"},"source":["\n","\n","More about the PCA class in scikit-learn can be found in its API documentation [here](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)."]},{"cell_type":"code","metadata":{"id":"Q0ujqO0sRN6N","executionInfo":{"status":"ok","timestamp":1711081420810,"user_tz":-60,"elapsed":2,"user":{"displayName":"Daniele Bonacorsi","userId":"01397661520201218305"}}},"source":["from sklearn.decomposition import PCA  # <---- note from where we take this module, too..."],"execution_count":24,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":false,"id":"KMEyi5QLRN6a","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1711081422792,"user_tz":-60,"elapsed":3,"user":{"displayName":"Daniele Bonacorsi","userId":"01397661520201218305"}},"outputId":"b2b180c5-5ca5-4170-941f-49f450f573e2"},"source":["# Feature Extraction with PCA\n","pca = PCA(n_components=3)\n","fit = pca.fit(X)\n","# summarize components\n","print(\"Explained Variance: %s\" % fit.explained_variance_ratio_)"],"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["Explained Variance: [0.889 0.062 0.026]\n"]}]},{"cell_type":"code","metadata":{"scrolled":true,"id":"aXKNLuSlRN6f","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1711081426229,"user_tz":-60,"elapsed":247,"user":{"displayName":"Daniele Bonacorsi","userId":"01397661520201218305"}},"outputId":"bd6a5e51-46e9-46ed-fc59-c2ff0ee5d7ca"},"source":["print(fit.components_)"],"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["[[-2.022e-03  9.781e-02  1.609e-02  6.076e-02  9.931e-01  1.401e-02\n","   5.372e-04 -3.565e-03]\n"," [-2.265e-02 -9.722e-01 -1.419e-01  5.786e-02  9.463e-02 -4.697e-02\n","  -8.168e-04 -1.402e-01]\n"," [-2.246e-02  1.434e-01 -9.225e-01 -3.070e-01  2.098e-02 -1.324e-01\n","  -6.400e-04 -1.255e-01]]\n"]}]},{"cell_type":"markdown","metadata":{"id":"dnqlWtnLRN6j"},"source":["You can see that the transformed dataset (3 principal components) bear little resemblance to the source data! It is a completely different approach, valuable mainly if you need to reduce the dimensions and the complexity of the problem."]},{"cell_type":"markdown","metadata":{"id":"55hqcH8iIB_I"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"tfv3IgWJRN6J"},"source":["### <font color='red'>Exercise</font>"]},{"cell_type":"markdown","metadata":{"id":"x7ZAW_XxRN6K"},"source":["Can you \"compare\" the first 3 methods above and draw any conclusions on the features you would eventually pick?"]},{"cell_type":"markdown","metadata":{"id":"A5bGAvBnIC0A"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"ihX3gbMNRN6k"},"source":["## Summary"]},{"cell_type":"markdown","metadata":{"id":"DUoPeiP5RN6k"},"source":["What we did:\n","\n","* we explored feature selection for preparing ML data in Python with scikit-learn, and we discovered 4 different automatic feature selection techniques."]},{"cell_type":"markdown","metadata":{"id":"5OuvxuzuRN6l"},"source":["## What's next"]},{"cell_type":"markdown","metadata":{"id":"_5iQYc30RN6m"},"source":["Now we will start looking at how to evaluate ML algorithms on your dataset, starting from discovering resampling methods that can be used to estimate the performance of a ML algorithm on unseen data."]}]}