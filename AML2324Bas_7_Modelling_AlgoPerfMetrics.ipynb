{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 2","language":"python","name":"python2"},"language_info":{"codemirror_mode":{"name":"ipython","version":2},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython2","version":"2.7.13"},"colab":{"provenance":[],"collapsed_sections":["UrZFXTsdckgO","NlR6mU7wcne2","dXrSnEVEH_hb","DFGrZupoH_hg","iBkJ2ZEq61vx","UX04UJW1H_hn","we42m9SZ8DIe","_HZhOq_NH_iA"]}},"cells":[{"cell_type":"markdown","metadata":{"id":"G-txJJP7H_hJ"},"source":["# ML Algorithm Performance Metrics"]},{"cell_type":"markdown","source":["The metrics that you choose to evaluate your ML algorithms are very important. Choice of metrics influences how the performance of ML algorithms is measured and compared. They influence how you weight the importance of different characteristics in the results and your ultimate choice of which algorithm to choose.\n","\n","We will discover how to select and use different ML performance metrics using (for the moment, still) only Python with *scikit-learn*."],"metadata":{"id":"D9EUEZohcJjq"}},{"cell_type":"markdown","metadata":{"id":"oarLVuEcH_hL"},"source":["## Algorithm Evaluation Metrics"]},{"cell_type":"markdown","metadata":{"id":"2M18Wz-HIk5G"},"source":["We will see various algorithm evaluation metrics and we will demonstrate them for both classification and regression type ML problems. In this section we need to focus on how to evaluate and compare algos themselves, so we need more models, and to do so we need more input datasets:\n","\n","We need more input datasets:\n","\n","* For CLASSIFICATION metrics, the Pima Indians onset of diabetes dataset is used as demonstration. This is a binary classification problem where all of the input variables are numeric.\n","* For REGRESSION metrics, we introduce the Boston House Price dataset and we use it as demonstration. This is a regression problem where all of the input variables are also numeric.\n"]},{"cell_type":"markdown","metadata":{"id":"we0oQqeDJSSZ"},"source":["We do not focus on modelling utself, in this notebook, so we use ***Logistic Regression*** for the classification problem and ***Linear Regression*** for the regression problems.\n","\n","A 10-fold CV test harness is used to demonstrate each metric (because this is a  likely scenario you will use when employing different algorithm evaluation metrics)\n","\n","More about ML algorithm performance metrics supported by scikit-learn can be found [here](http://scikit-learn.org/stable/modules/model_evaluation.html) on the page \"Model evaluation: quantifying the quality of predictions\"."]},{"cell_type":"markdown","source":["*CAVEAT. A caveat in these recipes is the `cross validation.cross_val_score` function (more [here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html) used to report the performance in each recipe. It does allow the use of different scoring metrics that will be discussed, but all scores are reported so that they can be sorted in ascending order (largest score is best). Some evaluation metrics (like mean squared error) are naturally descending scores (the smallest score is best) and as such are reported as negative by the cross validation.cross val score() function. This is important to note, because some scores will be reported as negative that by definition can never be negative. --- In other words, sklearn (from historical APIs) always tries to maximize scores, so loss functions (like MSE) have to be negated.*"],"metadata":{"id":"Y3UjOnIy-u-D"}},{"cell_type":"markdown","metadata":{"id":"S6L3iCx6H_hM"},"source":["# CLASSIFICATION Metrics"]},{"cell_type":"markdown","metadata":{"id":"Fy2LelSIPnS1"},"source":["## 0. Import the data"]},{"cell_type":"code","metadata":{"id":"D_0zvG3kPnJl"},"source":["import pandas as pd\n","\n","url = 'https://raw.githubusercontent.com/dbonacorsi/AMLBas2324/main/pima-indians-diabetes.data.csv'\n","\n","names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n","data = pd.read_csv(url, names=names)\n","data"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Classification problems are perhaps the most common type of ML problem and as such there is a myriad of metrics that can be used to evaluate predictions for these problems. In this section we will review 5 classification metrics.\n"],"metadata":{"id":"hMxX5_X3cbBi"}},{"cell_type":"markdown","metadata":{"id":"faE6yJgKH_hN"},"source":["Here they are:\n","* [CLAS-1] Classification Accuracy\n","* [CLAS-2] Logarithmic Loss\n","* [CLAS-3] Area Under ROC Curve\n","* [CLAS-4] Confusion Matrix\n","* [CLAS-5] Classification Report"]},{"cell_type":"markdown","metadata":{"id":"MrMybQseH_hO"},"source":["## [CLAS-1] Classification Accuracy"]},{"cell_type":"markdown","metadata":{"id":"24UleZkhH_hO"},"source":["Classification accuracy is **the number of correct predictions made as a ratio of all predictions made**.\n","\n","This is the most common evaluation metric for classification problems, and it is also often the most misused."]},{"cell_type":"markdown","source":["**It is really only suitable when there are an equal number of observations in each class** - which is rarely the case - and that all predictions and prediction errors are equally important - which is often not the case.\n"],"metadata":{"id":"-QP5Mgd1cki4"}},{"cell_type":"markdown","metadata":{"id":"8IHgnXlpPQY1"},"source":["Below is an example of calculating classification accuracy."]},{"cell_type":"code","metadata":{"id":"QIoQ8VxaH_hP"},"source":["from sklearn.model_selection import KFold\n","from sklearn.model_selection import cross_val_score\n","#\n","from sklearn.linear_model import LogisticRegression"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VsvmTIQNP8Oz"},"source":["array = data.values\n","X = array[:,0:8]\n","Y = array[:,8]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vwvS5XSLaxOc"},"source":["seed = 7"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZqctOAhTcTiJ"},"source":["We do k-fold CV."]},{"cell_type":"code","metadata":{"id":"8gx6xhKflHat"},"source":["kfold = KFold(n_splits=10, random_state=seed, shuffle=True)\n","model = LogisticRegression(solver='lbfgs', max_iter=5000)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"trt_zhuJH_hT"},"source":["# Cross Validation Classification Accuracy\n","scoring = 'accuracy'                                             # <---\n","results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n","print(\"Accuracy: %.3f (%.3f)\" % (results.mean(), results.std()))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"G7UZCPMcH_hW"},"source":["You can see that the accuracy ratio is reported: we built a model that is approximately 78% accurate."]},{"cell_type":"markdown","metadata":{"id":"UrZFXTsdckgO"},"source":["## <font color='red'>Exercise 1</font>"]},{"cell_type":"markdown","metadata":{"id":"TX5jWvreckGI"},"source":["Measure the time it takes to run the previous cell, by running k-fold CV with different k's, and compare timing and accuracies obtained."]},{"cell_type":"markdown","metadata":{"id":"NlR6mU7wcne2"},"source":["## <font color='green'>Solution 1</font>"]},{"cell_type":"code","metadata":{"id":"k7KKhaGCcmrq"},"source":["# enter your code here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BKNM45TQH_hX"},"source":["## [CLAS-2] Logarithmic Loss (aka \"logloss\")"]},{"cell_type":"markdown","metadata":{"id":"_jOB8f1-H_hX"},"source":["Logarithmic loss (or logloss) is **a performance metric for evaluating the predictions of probabilities of membership to a given class**."]},{"cell_type":"markdown","source":["The scalar probability between 0 and 1 can be seen as a measure of confidence for a prediction by an algorithm. Predictions that are correct or incorrect are rewarded or punished proportionally to the confidence of the prediction.\n"],"metadata":{"id":"EIj6Hjwmcrj0"}},{"cell_type":"markdown","metadata":{"id":"jlQw2gNbjO32"},"source":["Below is an example of calculating logloss for Logistic regression predictions on the Pima Indians onset of diabetes dataset.\n"]},{"cell_type":"code","metadata":{"id":"i2eytcFcH_hY"},"source":["# Cross Validation Classification LogLoss\n","scoring = 'neg_log_loss'                      #<---\n","results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n","print(\"Logloss: %.3f (%.3f)\" % (results.mean(), results.std()))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"feW0tkQDH_hb"},"source":["Smaller logloss is better, with 0 representing a perfect logloss. The\n","measure is inverted to be ascending when using the `cross_val_score()` function (see the documentation)."]},{"cell_type":"markdown","source":["\n","> Go to the slides for more examples and explanations..\n"],"metadata":{"id":"wRiNaWTm-TN2"}},{"cell_type":"markdown","metadata":{"id":"dXrSnEVEH_hb"},"source":["## [CLAS-3] Area Under ROC Curve"]},{"cell_type":"markdown","metadata":{"id":"5W0vSUv1H_hc"},"source":["Area under ROC Curve (or AUC for short) is **a performance metric for binary classification problems**.\n","\n","ROC can be broken down into **sensitivity** and **specificity**. A binary classification problem is really a trade-off between sensitivity and specificity.\n","* Sensitivity is the true positive rate (TPR) also called the Recall. It is the number of instances from the positive (first) class that actually predicted correctly.\n","* Specificity is also called the true negative rate (TNR). It is the number of instances from the\n","negative (second) class that were actually predicted correctly.\n","\n","The AUC represents a modelâ€™s ability to discriminate between positive and negative classes. An area of 1.0 represents a model that made all predictions perfectly. An area of 0.5 represents a model that is as good as random.\n","\n","The example below provides a demonstration of calculating AUC."]},{"cell_type":"code","metadata":{"id":"Ti20zvTkH_hc"},"source":["# Cross Validation Classification ROC AUC\n","scoring = 'roc_auc'\n","results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n","print(\"AUC: %.3f (%.3f)\" % (results.mean(), results.std()))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FxPGLPc6H_hf"},"source":["You can see the AUC is relatively close to 1 and greater than 0.5, suggesting some skills in the predictions."]},{"cell_type":"markdown","metadata":{"id":"DFGrZupoH_hg"},"source":["## [CLAS-4] Confusion Matrix"]},{"cell_type":"markdown","metadata":{"id":"AAU4XTpgH_hg"},"source":["The confusion matrix is **a handy (and more informative) presentation of the accuracy of a model with two or more classes**.\n","\n","The table presents predictions on the x-axis and accuracy outcomes on the y-axis. The cells of the table are the number of predictions made by a ML algorithm. See the frontal lectures for some examples.\n","\n","Below is an example of calculating a confusion matrix for a set of predictions by a Logistic Regression on the Pima Indians onset of diabetes dataset."]},{"cell_type":"markdown","source":["*E.g. a ML algorithm can predict 0 or 1 and each prediction may actually have been a 0 or 1. Predictions for 0 that were actually 0 appear in the cell for prediction = 0 and actual = 0, whereas predictions for 0 that were actually 1 appear in the cell for prediction = 0 and actual = 1. And so on.*"],"metadata":{"id":"IfolXWHz_DYo"}},{"cell_type":"markdown","metadata":{"id":"PUQWoyvGC6eC"},"source":["There are (at least) 2 different ways to do so\n","*   NOTE: to compare these two approaches and avoid to do mistakes, we need to re-execute (or just write again for clarity) some cells above - unnecessary if you do just one method, of course..\n"]},{"cell_type":"markdown","metadata":{"id":"Cq5wI17fIGjX"},"source":["### First method"]},{"cell_type":"markdown","metadata":{"id":"FGDPAwL8C-a1"},"source":["The first is not to rely on `cross_val_score` at all: there is no option to have a confusion matrix as scoring function in its call after having done the k-fold CV, so one way is not to do CV at all,  opt for a static splitting and validation, then use `confusion_matrix` directly."]},{"cell_type":"code","metadata":{"id":"KzVNaWxbDvaA"},"source":["from sklearn.model_selection import train_test_split      # <---\n","from sklearn.metrics import confusion_matrix              # <---"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ogz6JAG4C-FJ"},"source":["test_size = 0.33\n","\n","# Cross Validation Classification Confusion Matrix\n","X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size, random_state=seed)\n","model = LogisticRegression(solver='lbfgs', max_iter=500)\n","model.fit(X_train, Y_train)\n","predicted = model.predict(X_test)\n","matrix1 = confusion_matrix(Y_test, predicted)              # <---\n","print(matrix1)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-oYYi-T6H_hm"},"source":["Although the array is printed without headings, you can see that the majority of the predictions fall on the diagonal line of the matrix (which are correct predictions)."]},{"cell_type":"markdown","metadata":{"id":"WXzz84ZbIJWf"},"source":["### Second method"]},{"cell_type":"markdown","metadata":{"id":"ipuz93TsEMaz"},"source":["The second is keep doing k-fold CV, but to drop the use of `cross_val_score` in favour of `cross_val_predict`.\n"]},{"cell_type":"code","metadata":{"id":"MuWMtsTzH_hh"},"source":["from sklearn.model_selection import KFold                 # <---\n","from sklearn.model_selection import cross_val_predict     # <---\n","from sklearn.metrics import confusion_matrix              # <---"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6xtwfoCzBgRn"},"source":["kfold = KFold(n_splits=4, random_state=seed, shuffle=True)\n","model = LogisticRegression(solver='lbfgs', max_iter=300)\n","\n","predicted = cross_val_predict(model, X, Y, cv=kfold)    # <--- NOTE: no 'scoring'\n","matrix2 = confusion_matrix(Y, predicted)\n","print(matrix2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0OfnCOyaIlK6"},"source":["Same as above: the majority of the predictions fall on the diagonal line of the matrix. Good."]},{"cell_type":"markdown","metadata":{"id":"vbchSWFwHquU"},"source":["Let's make a couple of plots. We discuss later."]},{"cell_type":"code","metadata":{"id":"XoqjW7WYHK5-"},"source":["%matplotlib inline\n","import matplotlib.pyplot as plt\n","import seaborn as sn\n","#\n","df_cm = pd.DataFrame(matrix1)\n","plt.figure(figsize = (10,7))\n","sn.heatmap(df_cm, annot=True, cmap=\"YlOrRd\", fmt=\"d\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AauTmkozHe_Y"},"source":["%matplotlib inline\n","import matplotlib.pyplot as plt\n","import seaborn as sn\n","#\n","df_cm = pd.DataFrame(matrix2)\n","plt.figure(figsize = (10,7))\n","sn.heatmap(df_cm, annot=True, cmap=\"YlOrRd\", fmt=\"d\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iBkJ2ZEq61vx"},"source":["## <font color='red'>Exercise 2</font>"]},{"cell_type":"markdown","metadata":{"id":"kh6KgPD4Hj0Q"},"source":["The 2 matrices are not the same, though, aren't they? So: are the 2 results, content-wise, the same? or comparable?\n"]},{"cell_type":"markdown","metadata":{"id":"4UKxoLEZ637j"},"source":["## <font color='green'>Solution 2</font>"]},{"cell_type":"code","metadata":{"id":"FDFM48xA7IkV"},"source":["# write your code here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["> Go to the slides for a hint on the solution, and check the code below.."],"metadata":{"id":"89BRiX_idEIu"}},{"cell_type":"markdown","source":["A hint for the solution: in the slides, and in the code below."],"metadata":{"id":"Bbz-Up4aANh8"}},{"cell_type":"code","source":["### compare first and second method by changing matrix1 and matrix2 below\n","overall_size = 768.\n","test_size = 768.*0.33\n","train_size = overall_size - test_size\n","\n","print(\"Overall set size :\", overall_size)\n","print(\"Training set size :\", train_size)\n","print(\"Test set size :\", test_size)\n"],"metadata":{"id":"kTph7SboAP2k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tn, fp, fn, tp = matrix1.ravel()   #<--- here: do the same for matrix1 and matrix2\n","\n","print(tn+fp+fn+tp)"],"metadata":{"id":"irAdt9WvAZy4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tn, fp, fn, tp = matrix2.ravel()   #<--- here: do the same for matrix1 and matrix2\n","\n","print(tn+fp+fn+tp)"],"metadata":{"id":"QAUKJBCfAc4_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UX04UJW1H_hn"},"source":["## [CLAS-5] Classification Report"]},{"cell_type":"markdown","metadata":{"id":"qmB2Dqb_OY65"},"source":["There is also **a convenience report provided by the scikit-learn library** when working on classification problems to give you a quick idea of the accuracy of a model using a number of measures. The `classification report()` function displays the precision, recall, F1-score and support for each class.\n","\n","The example below demonstrates the report on the binary classification problem."]},{"cell_type":"code","metadata":{"id":"ds9M8G2NOb1q"},"source":["from sklearn.metrics import classification_report               # <---"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"x23ukIh1Ob1v"},"source":["test_size = 0.33\n","seed = 7\n","\n","# Cross Validation Classification Report\n","X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size,\n","    random_state=seed)\n","model = LogisticRegression(solver='lbfgs', max_iter=5000)\n","model.fit(X_train, Y_train)\n","predicted = model.predict(X_test)\n","report = classification_report(Y_test, predicted)               # <---\n","print(report)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fSm3BxC8OiaX"},"source":["You can see good prediction and recall for the algorithm."]},{"cell_type":"markdown","metadata":{"id":"d9hZrzyMH_hu"},"source":["# REGRESSION Metrics"]},{"cell_type":"markdown","metadata":{"id":"9tUWR8cj979e"},"source":["In the regression examples, we will use the Boston house price dataset, which you can find (original source) [here](https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data), and for your convenience it is already in the github repo of the course."]},{"cell_type":"markdown","metadata":{"id":"we42m9SZ8DIe"},"source":["## 0. Import the data"]},{"cell_type":"code","metadata":{"id":"F5BbQyy-8DIg"},"source":["import pandas as pd\n","\n","url = 'https://raw.githubusercontent.com/dbonacorsi/AML2324Bas/main/housing.data.csv'\n","\n","names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n","data = pd.read_csv(url, delim_whitespace=True, names=names)\n","data\n","\n","#array = dataframe.values\n","#X = array[:,0:13]\n","#Y = array[:,13]\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Rntb-1qWH_hv"},"source":["Here we will review 3 of the most common metrics for evaluating predictions on regression ML problems:\n","* [REGR-1] Mean Absolute Error\n","* [REGR-2] Mean Squared Error\n","* [REGR-3] $R^2$\n","\n"]},{"cell_type":"markdown","metadata":{"id":"SEl6bbYoH_hv"},"source":["## [REGR-1] Mean Absolute Error"]},{"cell_type":"markdown","metadata":{"id":"t13d5LAvH_hw"},"source":["The Mean Absolute Error (or MAE) is **the sum of the absolute differences between predictions and actual values**."]},{"cell_type":"markdown","source":["It gives an idea of how wrong the predictions were. The measure gives an idea of the magnitude of the error, but no idea of the direction (e.g. over or under predicting).\n"],"metadata":{"id":"aD5TYY_QdPt5"}},{"cell_type":"markdown","source":["It gives an idea of how wrong the predictions were. The measure gives an idea of the magnitude of the error, but no idea of the direction (e.g. over or under predicting)."],"metadata":{"id":"MkvrT9VACkEA"}},{"cell_type":"markdown","metadata":{"id":"fyzznaLS_rgP"},"source":["The example below demonstrates calculating mean absolute error on the house dataset."]},{"cell_type":"code","metadata":{"id":"1xoyhUzsH_hw"},"source":["from sklearn.model_selection import KFold\n","from sklearn.model_selection import cross_val_score\n","from sklearn.linear_model import LinearRegression"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tOPJHxVnH_h1"},"source":["# Cross Validation Regression MAE\n","kfold = KFold(n_splits=10, random_state=7, shuffle=True)\n","model = LinearRegression()\n","#\n","scoring = 'neg_mean_absolute_error'                                  # <---\n","results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n","print(\"MAE: %.3f (%.3f)\" % (results.mean(), results.std()))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yfhhJqhVH_h3"},"source":["A value of 0 indicates no error or perfect predictions. Like log-loss, this metric is inverted by\n","the `cross_val_score()` function."]},{"cell_type":"markdown","metadata":{"id":"k-pivqOUH_h4"},"source":["## [REGR-2] Mean Squared Error"]},{"cell_type":"markdown","metadata":{"id":"LL-RaegGH_h5"},"source":["The Mean Squared Error (or MSE) is much like the MAE in that **it provides a gross idea of the magnitude of error**."]},{"cell_type":"markdown","source":["Taking the square root of the mean squared error converts the units back to the original units of the output variable and can be meaningful for description and presentation. This is called the ***Root Mean Squared Error*** (or ***RMSE***). in Italian: (radice quadrata dell') errore quadratico medio."],"metadata":{"id":"3KhEQ1pNCu_L"}},{"cell_type":"markdown","metadata":{"id":"u5WMIXHeALJv"},"source":["\n","The example below provides a demonstration of calculating MSE."]},{"cell_type":"code","metadata":{"id":"AYdJ01hMH_h5"},"source":["#num_folds = 10       # a remnant..\n","kfold = KFold(n_splits=10, random_state=7, shuffle=True)\n","model = LinearRegression()\n","#\n","scoring = 'neg_mean_squared_error'\n","results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n","print(\"MSE: %.3f (%.3f)\" % (results.mean(), results.std()))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SalKxNvNH_h8"},"source":["This metric too is inverted so that the results are increasing."]},{"cell_type":"markdown","source":["Of course, remember to take the absolute value before taking the square root if you are interested in calculating the RMSE."],"metadata":{"id":"t-gJrvthC5Uz"}},{"cell_type":"markdown","metadata":{"id":"nBBBy7LfH_h8"},"source":["## [REGR-3] $R^2$ metric"]},{"cell_type":"markdown","metadata":{"id":"6tZ8RVQmH_h9"},"source":["The $R^2$ (or R Squared) metric provides **an indication of the goodness of fit of a set of predictions to the actual values**.\n"]},{"cell_type":"markdown","source":["In statistical literature this measure is called the coefficient of determination (in Italian: \"coefficiente di determinazione)\". This is a value between 0 and 1 for no-fit and perfect fit respectively."],"metadata":{"id":"9uxma6-PDAKK"}},{"cell_type":"markdown","metadata":{"id":"fWsHA29oAlky"},"source":["\n","The example below provides a demonstration of calculating the mean $R^2$ for a set of predictions."]},{"cell_type":"code","metadata":{"id":"r21teffhH_h9"},"source":["# Cross Validation Regression R^2\n","kfold = KFold(n_splits=10, random_state=7, shuffle=True)\n","model = LinearRegression()\n","#\n","scoring = 'r2'                                                     # <---\n","results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n","print(\"R^2: %.3f (%.3f)\" % (results.mean(), results.std()))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YPPk368jH_iA"},"source":["You can see the predictions have a poor fit to the actual values with a value closer to zero and less than 0.5."]},{"cell_type":"markdown","metadata":{"id":"_HZhOq_NH_iA"},"source":["## Summary"]},{"cell_type":"markdown","metadata":{"id":"7qksn4XwH_iB"},"source":["What we did:\n","\n","* we discovered metrics that you can use to evaluate your ML algorithms. We learned about 3 classification metrics (Accuracy, LogLoss and AUC) and 2 convenience methods for classification prediction results (Confusion Matrix and Classification Report), as well as 3 metrics for regression problems (MAE, MSE, R2)."]}]}