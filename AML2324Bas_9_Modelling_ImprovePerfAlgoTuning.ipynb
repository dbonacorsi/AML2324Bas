{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 2","language":"python","name":"python2"},"language_info":{"codemirror_mode":{"name":"ipython","version":2},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython2","version":"2.7.13"},"colab":{"provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"TdLnzL-8VYg2"},"source":["# Improve Performance with Algorithm Tuning"]},{"cell_type":"markdown","metadata":{"id":"JpZsPEujVYg4"},"source":["**ML models are parameterized** so that the behavior of an algorithm trained on some data can be tuned for a given problem. Models can have so many parameters, and **finding the best combination of parameters** is an optimization task and can be treated as a search problem (more later).\n","\n","NOTE the jargon: here we are using the term \"parameters\" in its wider sense..\n","\n","We will see how to tune the parameters of ML algorithms in Python using scikit-learn.\n","\n","The goal is to learn:\n","1. The importance of algorithm parameter tuning to improve algorithm performance\n","2. How to use a grid search algorithm tuning strategy\n","3. How to use a random search algorithm tuning strategy."]},{"cell_type":"markdown","metadata":{"id":"VlCigFSTVYg5"},"source":["# ML Algorithm hyper-parameters optimization"]},{"cell_type":"markdown","metadata":{"id":"ADK0eDXWVYg7"},"source":["Algorithm tuning is a final step in the process of applied ML before finalizing your model. It is sometimes called ***hyperparameter optimization*** where:\n","\n","* the _algorithm parameters_ are referred to as **hyperparameters**\n","* the _coefficients_ found by the ML algorithm itself are referred to as **parameters**.\n","\n","Optimization suggests the search-nature of the problem. Phrased as a search problem, you can use different search strategies to find a good and robust parameter or set of parameters for an algorithm on a given problem.\n","\n","Python scikit-learn provides 2 simple methods for algorithm parameter tuning:\n","1. Grid Search parameter tuning\n","2. Random Search parameter tuning."]},{"cell_type":"markdown","metadata":{"id":"USjW1RvPW07I"},"source":["## 0. Import the data"]},{"cell_type":"code","metadata":{"id":"m32ckL3sW0wt"},"source":["import pandas as pd\n","\n","url = 'https://raw.githubusercontent.com/dbonacorsi/AMLBas2324/main/pima-indians-diabetes.data.csv'\n","\n","names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n","data = pd.read_csv(url, names=names)\n","data"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lhvddoOsVYg-"},"source":["## 1. Grid Search Parameter Tuning"]},{"cell_type":"markdown","metadata":{"id":"FivKEQX0VYg_"},"source":["Grid search is an approach to parameter tuning that will **methodically build and evaluate a model for each combination of algorithm parameters specified in a grid**.\n","\n","You can perform a grid search using the GridSearchCV class (documented [here](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)).\n","\n","The example below evaluates different alpha values for the Ridge Regression algorithm on the standard diabetes dataset. This is a one-dimensional grid search."]},{"cell_type":"code","metadata":{"id":"8-sb9YjPVYhK"},"source":["import numpy as np\n","#\n","#from pandas import read_csv\n","#\n","from sklearn.linear_model import Ridge\n","#\n","from sklearn.model_selection import GridSearchCV                   # <--"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZSarf4FZVYiT"},"source":["array = data.values\n","X = array[:,0:8]\n","Y = array[:,8]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"bAos0D0LVYig"},"source":["# Grid Search for Algorithm Tuning\n","alphas = np.array([1., 0.1, 0.01, 0.001])\n","my_param_grid = dict(alpha=alphas)\n","model = Ridge()\n","grid = GridSearchCV(estimator=model, param_grid=my_param_grid, cv=100)\n","grid.fit(X, Y)\n","print(grid.best_score_)\n","print(grid.best_estimator_.alpha)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Examine the code above carefully:\n","\n","*   what is `alphas`?\n","*   what is `alpha`?\n","*   what is `my_param_grid`?\n","*   what is `param_grid`?\n"],"metadata":{"id":"SmRYy6-iaEf0"}},{"cell_type":"markdown","metadata":{"id":"jezfluyqVYij"},"source":["Running the example lists out the optimal score achieved and the set of parameters in the\n","grid that achieved that score. In this case ap optimal `alpha` among those explicitly given is found."]},{"cell_type":"markdown","metadata":{"id":"XrjG_ElKZw-o"},"source":["## <font color='red'>Exercise 1</font>"]},{"cell_type":"markdown","metadata":{"id":"SMZb8HVOZw6Q"},"source":["Can you find better values than the one found above? How would you do it?\n","\n"]},{"cell_type":"markdown","metadata":{"id":"a_4DSjaSZwun"},"source":["## <font color='green'>Solution 1</font>"]},{"cell_type":"code","metadata":{"id":"ShoiNulTZwTM"},"source":["# put your code here"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hDebgFIaVYik"},"source":["## Random Search parameter tuning"]},{"cell_type":"markdown","metadata":{"id":"x1IgugYyVYik"},"source":["Random search is an approach to parameter tuning that will **sample algorithm parameters from a random distribution (i.e. uniform) for a fixed number of iterations**.\n","\n","A model is constructed and evaluated for each combination of parameters chosen. You can perform a random search for algorithm parameters using the `RandomizedSearchCV` class (documented [here](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html)).\n","\n","Suppose that you want to go slow, your algo does not impose time constraints in training time, so you do not require an optimal, but perhaps too aggressive, alpha. How do you find a value in the [0,1] interval?\n"]},{"cell_type":"markdown","source":["Examine again the code above."],"metadata":{"id":"knXvTgLoajmU"}},{"cell_type":"markdown","metadata":{"id":"ws_Cni8YALaL"},"source":["Well, sure, you found `1.0` is optimal, but just among the ones you typed. Which is the best floating point value in the [0,1] interval is still unknown.. and you cannot type manually all real values, of course! So, what?\n","\n","The example below evaluates different random alpha values between 0 and 1 for the Ridge Regression algorithm on the standard diabetes dataset. A total of 100 iterations are performed with uniformly random alpha values selected in the range between 0 and 1 (the range that alpha values can take)."]},{"cell_type":"code","metadata":{"id":"6lFraTa8VYil"},"source":["from scipy.stats import uniform\n","#\n","from sklearn.linear_model import Ridge\n","#\n","from sklearn.model_selection import RandomizedSearchCV                   # <--"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RuB0_9b7VYis"},"source":["%%time\n","# Randomized for Algorithm Tuning\n","my_param_grid = { 'alpha' : uniform()}\n","model = Ridge()\n","rsearch = RandomizedSearchCV(estimator=model, param_distributions=my_param_grid, n_iter=10, random_state=7)\n","rsearch.fit(X, Y)\n","print(rsearch.best_score_)\n","print(rsearch.best_estimator_.alpha)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NCASRtmiVYiv"},"source":["Running the example produces results much like those in the grid search example above. An\n","optimal `alpha` value near `1.0` is discovered."]},{"cell_type":"markdown","source":["How long it takes, are you happy with the result? Can you do better? How? Play with `n_iter` and check the time it takes.."],"metadata":{"id":"IuLnwEwvbHPA"}},{"cell_type":"code","source":["%%time\n","# Randomized for Algorithm Tuning\n","my_param_grid = { 'alpha' : uniform()}\n","model = Ridge()\n","rsearch = RandomizedSearchCV(estimator=model, param_distributions=my_param_grid, n_iter=1000, random_state=7)\n","rsearch.fit(X, Y)\n","print(rsearch.best_score_)\n","print(rsearch.best_estimator_.alpha)"],"metadata":{"id":"HWqO3jn6bUUc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SS76zGlRVYiw"},"source":["## Summary"]},{"cell_type":"markdown","metadata":{"id":"cYHAUIXWVYix"},"source":["What we did:\n","\n","* we discovered that algorithm parameter tuning is an important step for improving algorithm performance right before presenting results or preparing a system for production. We explored two methods that you can use right now in Python and scikit-learn to improve your algorithm results (Grid Search Parameter Tuning, Random Search Parameter Tuning)."]}]}